from fastapi import FastAPI, UploadFile, File, Form
from fastapi.responses import FileResponse, HTMLResponse
import pandas as pd
import openai
import tempfile
import os
import re

app = FastAPI(
    title="TAAA Semantic Extractor",
    description="Adaptive multilingual semantic keyword extractor using GPT-4o-mini",
    version="4.0.0"
)

openai.api_key = os.getenv("OPENAI_API_KEY")

@app.get("/", response_class=HTMLResponse)
def home():
    html = open("index.html", "r", encoding="utf-8").read()
    return HTMLResponse(content=html)

@app.post("/analyze_csv")
async def analyze_csv(file: UploadFile = File(...), bilingual: str = Form("false")):
    df = pd.read_csv(file.file)
    if "abstract" not in df.columns:
        return {"error": "Missing 'abstract' column."}

    bilingual_mode = (bilingual.lower() == "true")

    results = df["abstract"].apply(lambda x: extract_keywords_global(x, bilingual_mode))
    df["language"], df["keywords"] = zip(*results)

    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".csv")
    df.to_csv(tmp.name, index=False, encoding="utf-8-sig")
    return FileResponse(tmp.name, media_type="text/csv", filename="taaa_keywords_global.csv")

# ğŸŒ Language detection
def detect_language(text: str) -> str:
    zh_count = len(re.findall(r'[\u4e00-\u9fff]', text))
    en_count = len(re.findall(r'[A-Za-z]', text))
    jp_count = len(re.findall(r'[\u3040-\u30ff]', text))
    kr_count = len(re.findall(r'[\uac00-\ud7af]', text))
    accented = len(re.findall(r'[Ã¡Ã©Ã­Ã³ÃºÃ¼Ã±Ã§Ã Ã¨Ã¹ÃŸÎ±Î²Î³Ğ°Ğ±Ğ²Ğ³]', text, flags=re.IGNORECASE))

    if zh_count > 0 and en_count > 0:
        return "mixed"

    lang_scores = {
        "chinese": zh_count,
        "english": en_count,
        "japanese": jp_count,
        "korean": kr_count,
        "other": accented
    }
    lang = max(lang_scores, key=lang_scores.get)
    return lang if lang_scores[lang] > 0 else "unknown"

# ğŸ§  Optimized multilingual prompt templates
PROMPTS = {
    "chinese": "è«‹å¾ä»¥ä¸‹ä¸­æ–‡æ‘˜è¦ä¸­èƒå–10å€‹å…·èªç¾©ä»£è¡¨æ€§çš„å­¸è¡“é—œéµè©ï¼Œå¼·èª¿ç ”ç©¶ä¸»é¡Œã€æ–¹æ³•èˆ‡æ ¸å¿ƒæ¦‚å¿µã€‚ä»¥é “è™Ÿï¼ˆã€ï¼‰åˆ†éš”ã€‚è‹¥ç‚ºæŠ€è¡“åè©ï¼Œè«‹ä¿æŒåŸæ–‡æˆ–ä»¥è‹±æ–‡æ¨™ç¤ºã€‚\n\næ‘˜è¦ï¼š\n{text}",
    "english": "Extract 10 semantically representative academic keywords from the following English abstract. Focus on scientific themes, methods, and key terminology. Separate keywords by commas.\n\nAbstract:\n{text}",
    "japanese": "æ¬¡ã®æ—¥æœ¬èªã®è¦ç´„ã‹ã‚‰ã€ç ”ç©¶ã®ãƒ†ãƒ¼ãƒã€æ–¹æ³•ã€ä¸»è¦ãªæ¦‚å¿µã‚’è¡¨ã™ä»£è¡¨çš„ãªå­¦è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’10å€‹æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚è‹±èªã®å°‚é–€ç”¨èªãŒå¿…è¦ãªå ´åˆã¯ä½µè¨˜ã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯èª­ç‚¹ï¼ˆã€ï¼‰ã§åŒºåˆ‡ã£ã¦ãã ã•ã„ã€‚\n\nè¦ç´„ï¼š\n{text}",
    "korean": "ë‹¤ìŒ í•œêµ­ì–´ ì´ˆë¡ì—ì„œ ì—°êµ¬ ì£¼ì œ, ë°©ë²•, í•µì‹¬ ê°œë…ì„ ëŒ€í‘œí•˜ëŠ” í•™ìˆ ì  ì£¼ìš” í‚¤ì›Œë“œ 10ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. í•„ìš”í•  ê²½ìš° ì˜ì–´ ê¸°ìˆ  ìš©ì–´ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”. í‚¤ì›Œë“œëŠ” ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.\n\nì´ˆë¡:\n{text}",
    "spanish": "Extrae 10 palabras clave acadÃ©micas representativas del siguiente resumen en espaÃ±ol. EnfÃ³cate en el tema de investigaciÃ³n, metodologÃ­a y conceptos principales. Separa las palabras clave con comas. Si existen tÃ©rminos tÃ©cnicos, puedes mantenerlos en inglÃ©s.\n\nResumen:\n{text}",
    "french": "Extrayez 10 mots-clÃ©s acadÃ©miques reprÃ©sentatifs du rÃ©sumÃ© suivant en franÃ§ais. Mettez lâ€™accent sur le sujet de recherche, la mÃ©thode et les concepts clÃ©s. SÃ©parez les mots-clÃ©s par des virgules. Les termes techniques peuvent rester en anglais.\n\nRÃ©sumÃ© :\n{text}",
    "mixed": "The following abstract contains both Chinese and English text. Please extract 10 representative academic keywords in English only, summarizing the research focus and technical themes. Separate by commas.\n\nAbstract:\n{text}",
    "other": "The following abstract is written in {language}. Extract 10 representative academic keywords in the same language if possible. If technical or scientific, provide English equivalents in parentheses. Separate keywords by commas or the natural punctuation of the language.\n\nAbstract:\n{text}",
    "bilingual": "è«‹æ ¹æ“šä»¥ä¸‹æ‘˜è¦ï¼Œåˆ†åˆ¥ä»¥ç¹é«”ä¸­æ–‡èˆ‡è‹±æ–‡å„èƒå–10å€‹å…·èªç¾©ä»£è¡¨æ€§çš„å­¸è¡“é—œéµè©ã€‚è«‹è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š\nä¸­æ–‡é—œéµè©ï¼š...(ä»¥é “è™Ÿã€Œã€ã€åˆ†éš”)\nEnglish keywords: ...(ä»¥é€—è™Ÿã€Œ,ã€åˆ†éš”)\n\næ‘˜è¦å…§å®¹ï¼š\n{text}"
}

# âœ¨ Core function
def extract_keywords_global(text, bilingual_mode=False):
    if not text or pd.isna(text):
        return ("unknown", "")

    lang = detect_language(text)
    prompt = ""

    if bilingual_mode:
        prompt = PROMPTS["bilingual"].format(text=text)
    else:
        if lang in PROMPTS:
            prompt = PROMPTS[lang].format(text=text)
        else:
            prompt = PROMPTS["other"].format(language=lang, text=text)

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2
        )
        keywords = response["choices"][0]["message"]["content"].strip()
        return (lang, keywords)
    except Exception as e:
        return (lang, f"Error: {e}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 10000))
    uvicorn.run(app, host="0.0.0.0", port=port)
